\section{Model}
\subsection{Jetlag}
The suprachiasmatic nucleus (SCN) is a mammalian region of the hypothalamus that contains around 10,000 neurons. The firing of vasoactive intestinal polypeptide indicates rhythm in individual neurons of SCN. Thus, it is responsible for circadian rhythms, which cyclically control sleep, physical activity, alertness, hormone levels, body temperature, immune function, and digestive activity. These factors contribute to productivity and mood, and therefore, it is essential for the neurons of SCN to be in synchrony with environmental conditions of the time zone. Because circadian rhythm is cyclic, we can model neurons in the SCN with individual oscillators.

A reputable dynamical system for modeling multiple oscillators is the Kuramoto Model, given by
$$\dot{\theta_i} = \omega_i + \frac{k}{n} \sum_{j=1}^ {N}(\sin{\theta_j - \theta_i}) + \zeta_i$$

where each $\theta_i$ is an oscillator, and $\omega_i$ is the natural frequency of oscillator $\theta_i$. $\zeta_i$ is simply a noise function to be adjusted by user. In this model, we added a noise function that would model daylight. The function was: 
$$\zeta_i = F \sin(\sigma t - \theta_i + \Delta(p))$$ 

where $\Delta(p)$ is the radial longitudinal difference in position after travel. 
If $\Delta(p)$ is a negative value, we see that $\zeta_1$ has a more significant effect. This subtlety results in significant differences between eastward and westward jet lag. 
This model, like all kuromoto models, has the complex part of 
$$R = \frac{1}{N} \sum_{j=1}^N (e^{\theta_j - \sigma t - \Delta(p)})$$

which models the average value of all $\theta_i$. Here we subtract $\sigma t+ \Delta(p)$ to account for the changes in starting position. Childs and Strogatz showed that we can model system $\dot{R}$ using 
$$\dot{R} = \frac{(KR + F) - R^2(\overline{KR + F})}{2} - (\Delta + i\Omega)R$$
Where $K, F, \Delta, \Omega$ are constants dependent on values of $N, K$ selected.
The zeroes of this equation are the stability points of $\dot{R}$, which we denote as $R_{sp}$.
So, the starting points of $\theta_i$ are centered about:
$$R_{sp} \cdot \mathrm{rotation}$$
In this case, the rotation aroound the origin corresponds to the $\Delta(p)$ that the plane travels. 
Using a population density function, we then randomly sample points around this starting point to generate all $\theta_i$.
This model's algorithm can be found in subsection 4.1.

\subsection{Spherical Distance}
$$\vec{v_1} \cdot \vec{v_2} = \|\vec{v_1}\| \|\vec{v_2}\| \cos(\theta)$$
where $\theta$ is the angle in between the two vectors, also the distance between two points on a unit sphere. We use this to find spherical distance between points. Given longitude and lattitude, we convert coordinates to unit vectors using the formulas:
$$x = \cos(\mathrm{lat}) \cdot \cos(\mathrm{long})$$. We then get:
$$\vec{v_1} \cdot \vec{v_2} = 1 \cdot 1 \cdot \cos(\theta)$$
$$\arccos({\vec{v_1} \cdot \vec{v_2}}) = \arccos(\cos(\theta))$$
$$\theta = \arccos(\vec{v_1} \cdot \vec{v_2})$$
This model's algorithm can be found in subsection 4.2.

\subsection{Cost}
Although not a major factor, we consider cost. To determine cost function, we analyze flight data from LAX to Tokyo NRT using SkyScanner's API. Data include date of departure and cost. We used this data to determine a function of cost in terms of departure date.




As air fare changes throughout the year, we consider cost based on time of year. We analyzed international flight data from LAX to Tokyo NRT, which include time of departure and date. We used this information to regress a polynomial $f(t)$, where $t$ is departure date. $f(t)$ had an $R^2$ value of 0.6890. Mean cost is interpreted as
$$m(t) = \frac{1}{365} \int_1^{365}f(t) \mathrm{d} t $$. 


To estimate mean cost based on distance, we use 1,780,832 price points from Rome2Rio.com. Airfare cost follows a linear graph with the equation $$C(x) = 50 + (x \cdot 0.11)$$ where C(d) is the cost in USD of flying $x$ miles. Then, $$C(d) = 50 + (d \cdot  0.177)$$ where C(d) is the cost in USD of flying $d$ kilometers. 
We multiply our first function by our second function to get a cost function based on flight distance and departure date, which we define as $C_m(d,t)$.

\subsubsection{Multi-variable linear regression and Polynomial regression}
The general hypothesis for a linear model of $n$ variables is
$$h_{\theta}(x_0, x_1, x_2, \dots, x_n) = \theta_0 \cdot x_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \dots  \theta_n \cdot x_n$$
We write all $\theta_i$ in a matrix:
$$
\theta = 
\begin{bmatrix}
\theta_0 & \theta_1 & \theta_2 & \dots & \theta_n 
\end{bmatrix}
\in \mathbb{R}
$$
and all $x_i$ in a matrix:
$$
x = 
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots\\
x_n
\end{bmatrix}
\in {\mathbb{R}}^{n+1}
$$

We write
$$h(x_0, x_1, x_2, \dots, x_n) = \theta \cdot x \\
= 
\begin{bmatrix}
\theta_0 & \theta_1 & \theta_2 & \dots & \theta_n 
\end{bmatrix}
\cdot
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots\\
x_n
\end{bmatrix} 
$$

$$= \theta_0 \cdot x_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \dots + \theta_n \cdot x_n$$

for $i$ input values we model input/output as the following:
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
\hline
$m$ & $x_0$ & $x_1$ & $x_2$ & $x_3$ & \dots & $x_n$ & y \\
\hline
1 & $x_0^{(1)}$ & $x_1^{(1)}$ & $x_2^{(1)}$ & $x_3^{(1)}$ & $\dots$ & $x_n^{(1)}$ & $y^{(1)}$\\ 
2 & $x_0^{(2)}$ & $x_1^{(2)}$ & $x_2^{(2)}$ & $x_3^{(2)}$ & $\dots$ & $x_n^{(2)}$ & $y^{(2)}$\\ 
3 & $x_0^{(3)}$ & $x_1^{(3)}$ & $x_2^{(3)}$ & $x_3^{(3)}$ & $\dots$ & $x_n^{(3)}$ & $y^{(3)}$\\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\ 
m & $x_0^{(m)}$ & $x_1^{(m)}$ & $x_2^{(m)}$ & $x_3^{(m)}$ & \dots & $x_n^{(m)}$ & $y^{(m)}$\\ 
\hline
\end{tabular}
\end{center} 

Here, $x^{(0)}_m$ are the inputs of $\theta_m(x)$. For instance, 
$$x^{(2)} =  
\begin{bmatrix}
x_0^{(2)} \\
x_1^{(2)} \\
x_2^{(2)} \\
x_3^{(2)} \\
\vdots \\
x_n^{(2)}
\end{bmatrix}
\in \mathbb{R}^{n+1}
$$

We use average mean square difference for cost function of regression, which is given by
$$j(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)} - y^{(i)})^{2})$$


\subsubsection{Gradient descent}
We optimize the formula given above using a technique known as gradient descent.
The general algorithm for gradient descent is 
$$\theta_i \coloneqq \theta_i - \alpha \frac{\partial}{\partial \theta_i}(j(\theta))$$
where $j(\theta)$ is a cost function and $\alpha$ is the learning rate.
For multiple variables, algorithm is simultaneously evaluated for all $\theta_i$. 
For example, a ten variable equation would be regressed using:
$$\theta_0 \coloneqq \theta_0 - \alpha \frac{\partial}{\partial \theta_0}(j(\theta))$$
$$\theta_1 \coloneqq \theta_1 - \alpha \frac{\partial}{\partial \theta_1}(j(\theta))$$
$$\theta_2 \coloneqq \theta_2 - \alpha \frac{\partial}{\partial \theta_2}(j(\theta))$$
$$\vdots$$
$$\theta_{10} \coloneqq \theta_{10} - \alpha \frac{\partial}{\partial \theta_{10}}(j(\theta))$$
When we substitute in $j(\theta)$, we get
$$\theta_i \coloneqq \theta_{i} - \alpha \frac{\partial}{\partial \theta_i}(\frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})))$$
which simplifies to
$$\theta_j = \theta_j - \alpha \frac{1}{m} (h_{\theta}^{(i)} - y^{(i)}) \cdot x_j^{(i)}$$

\subsubsection{Gradient descent in polynomial regression}
We use the same input/output and algorithm in multi-variable linear regression, but we use 1 input and create more variables by increasing its power. The input would be as follows  
\begin{center}
\begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
\hline
$m$ & $x^0$ & $x^1$ & $x^2$ & $x^3$ & \dots & $x^n$ & y \\
\hline
1 & $(x^{(1)})^0$ & $x^{(1)}$ & $(x^{(1)})^2$ & $(x^{(1)})^3$ & $\dots$ & $(x^{(1)})^n$ & $y^{(1)}$\\ 
2 & $(x^{(2)})^0$ & $x^{(2)}$ & $(x^{(2)})^2$ & $(x^{(2)})^3$ & $\dots$ & $(x^{(2)})^n$ & $y^{(2)}$\\
3 & $(x^{(3)})^0$ & $x^{(3)}$ & $(x^{(3)})^2$ & $(x^{(3)})^3$ & $\dots$ & $(x^{(3)})^n$ & $y^{(3)}$\\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$\\ 
m & $(x^{(m)})^0$ & $x^{(m)}$ & $(x^{(m)})^2$ & $(x^{(m)})^3$ & \dots & $(x^{(m)})^n$ & $y^{(m)}$\\ 
\hline
\end{tabular}
\end{center} 


\newpage


\section{Model Implementation}
\subsection{Kuramoto Model}
We now find algorithm to implement model in Section 3.1. First, we solve zeroes of system $\dot{R}$, given by equation,
$$\dot{R} = \frac{(KR + F) - R^2(\overline{KR + F})}{2} - (\Delta + i\Omega)R$$
Since starting values of $\theta_i$, which is $\theta_i(0)$, and $\frac{\mathrm{d} \theta_i}{\mathrm{d} t}$ are calculatable, we use Euler's method to find $\theta_i(0 + \mathrm{d}t)$. 
The algorithm is as follows:
\begin{center}
\begin{algorithm}[h]
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKw{Calculate}{calculate}
\SetKw{Set}{set}
\SetKw{Return}{return}

\Input{$R_{st}$, $\mathrm{d} t$}
 \Set{$t = 0$}\;
 initialization\;
 \Calculate{initial values of all $\theta_i$}\;
 \While{$|R - R_{st}|$ is decreasing}{
  \
  \For{each $\theta_i$}{
   \Calculate{$\frac{\mathrm{d} \theta_i}{\mathrm{d} t}$}\;
   
   \Set{$\theta_i = \theta_i + \frac{\mathrm{d} \theta_i}{\mathrm{d} t} \cdot \mathrm{d} t$}\;
  }
 \Set{$t = t + \mathrm{d} t$}\;
 \Calculate{new $R$}\;
 \Return{$|R - R_{st}|$}\;
 }
 \caption{Using Euler's method to find relative jetlag}
\end{algorithm}
\end{center}
